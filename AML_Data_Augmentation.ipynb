{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RefineX/Change-Detection/blob/main/AML_Data_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing libraries & mounting google drive"
      ],
      "metadata": {
        "id": "b2tFhZVfCDi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install image-classifiers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gRRiXuberuln",
        "outputId": "b54170ac-8a95-4134-8d57-397d69173fba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting image-classifiers\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (3.1.0)\n",
            "Installing collected packages: keras-applications, image-classifiers\n",
            "Successfully installed image-classifiers-1.0.0 keras-applications-1.0.8\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Downloader\n",
        "dataset = \"LEVIRCD_Plus\" #@param [\"S2Looking\", \"LEVIRCD_Plus\"]\n",
        "resize = \"512\" #@param [\"1024\", \"512\", \"256\"]\n",
        "crop = \"256\" #@param [\"1024\", \"512\", \"256\"]\n",
        "\n",
        "data_link_dict = {\n",
        "    'S2Looking': {\n",
        "      '1024_512': \"1-0_vcODMYmyYIY_uhMcs97aAk2OJVduq&confirm=t\",\n",
        "      '1024_256': \"1-0mVd6BjKnG3LXhYkbyxdx0-dtRKY6KO&confirm=t\",\n",
        "      '512_512': \"1-6WOmE0LTJ4Z31EphmA0L8QY5NMMllsT&confirm=t\",\n",
        "      '512_256': \"1-7NSOmHTsDpkEtbgt_N4VSgipCo5kr3a&confirm=t\",\n",
        "      '256_256': \"1-8x5d5DrNsgJ5eAOYyKwi91g8knGeRjQ&confirm=t\",\n",
        "      '1024_1024': \"1GzrgMwJKguXSWSfFsBSC2qSr52fVEc7W&confirm=t\"\n",
        "    },\n",
        "    'LEVIRCD_Plus': {\n",
        "      '1024_512': \"1-22GjfF8mlLFNyoJaa6_GMJRfTCoGZyc&confirm=t\",\n",
        "      '1024_256': \"1-2r-zCCfQjLRtwwXMGPYAxEJcTS8Hp80&confirm=t\",\n",
        "      '512_512': \"1-43scZrxe3Q_PH2EnBRuc_6l9O9WjPs2&confirm=t\",\n",
        "      '512_256': \"1-5oC0xV36S4K5VMiQuwheWoyMt1ymYb9&confirm=t\",\n",
        "      '256_256': \"1-69cdgqlcXunt5vrCR9jRvcMkpPNzYWr&confirm=t\",\n",
        "      '1024_1024': \"1nyPJZGGOL7o8A0m0rGw7wyu2BFIxC4SD&confirm=t\"\n",
        "    }\n",
        "}\n",
        "\n",
        "import os\n",
        "if os.path.exists(f'Data/{dataset}/{resize}_{crop}'):\n",
        "  print('This dataset already exists.')\n",
        "else:\n",
        "  gdown_link = data_link_dict[dataset][f'{resize}_{crop}']\n",
        "  !gdown \"{gdown_link}\"\n",
        "  print('Unzipping...',end='')\n",
        "  !unzip -q \"{resize}_{crop}.zip\"\n",
        "  print('Done.\\nDeleting zip...',end='')\n",
        "  !rm \"{resize}_{crop}.zip\"\n",
        "  print('Done.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jdXkHvK6gGr7",
        "outputId": "af0b8057-a075-4e32-f065-372074b9c2a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-5oC0xV36S4K5VMiQuwheWoyMt1ymYb9&confirm=t\n",
            "To: /content/512_256.zip\n",
            "100% 317M/317M [00:02<00:00, 127MB/s]\n",
            "Unzipping...Done.\n",
            "Deleting zip...Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing dependencies"
      ],
      "metadata": {
        "id": "vpQO0RMjCHpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "import cv2\n",
        "import pywt\n",
        "import random\n",
        "import shutil as sh"
      ],
      "metadata": {
        "id": "B9GQ_AA1i5Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Characteristics\n",
        "dataset = \"LEVIRCD_Plus\" #@param [\"S2Looking\", \"LEVIRCD_Plus\"]\n",
        "resized_size = \"512\" #@param [1024, 512, 256]\n",
        "crop_size = \"256\" #@param [1024, 512, 256]\n",
        "resized_size = int(resized_size)\n",
        "crop_size = int(crop_size)\n",
        "if dataset == 'S2Looking':\n",
        "  pre_image = 'Image1'\n",
        "  post_image = 'Image2'\n",
        "elif dataset == 'LEVIRCD_Plus':\n",
        "  pre_image = 'A'\n",
        "  post_image = 'B'\n",
        "label_image = 'label'\n",
        "PATH = f\"/content/Data/{dataset}/{resized_size}_{crop_size}\"\n",
        "Path('saved/models/').mkdir(parents=True, exist_ok=True)\n",
        "Path('saved/histories/').mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "AFi5x75Ek8IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subset selection for few-shot learning"
      ],
      "metadata": {
        "id": "SbXNRmYcCQHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset Choice\n",
        "subset_percent = \"10\" #@param [\"10\", \"25\", \"50\", \"100\"]\n",
        "if subset_percent == \"100\":\n",
        "  PATH = PATH\n",
        "else:\n",
        "  Subset_PATH = PATH+\"_subset_{}\".format(subset_percent)\n",
        "  Path(Subset_PATH).mkdir(parents=True, exist_ok=True)\n",
        "  dirs = os.listdir(PATH)\n",
        "  subdirs = os.listdir(PATH+\"/\"+dirs[0])\n",
        "  for dir in dirs:\n",
        "    for subdir in subdirs:\n",
        "      Path(Subset_PATH+\"/\"+dir+\"/\"+subdir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  for dir in dirs:\n",
        "    img_list = os.listdir(PATH+\"/\"+dir+\"/\"+pre_image)\n",
        "    initial_count = len(img_list)\n",
        "    final_count = int(initial_count * int(subset_percent)/100)\n",
        "    random.seed(0)\n",
        "    subset_img_list = random.sample(img_list, final_count)\n",
        "    for subdir in subdirs:\n",
        "      for subset_img in subset_img_list:\n",
        "        if subdir == label_image:\n",
        "          source_path = PATH+\"/\"+dir+\"/\"+subdir+\"/\"+subset_img.replace('jpg','png')\n",
        "          destination_path = Subset_PATH+\"/\"+dir+\"/\"+subdir+\"/\"+subset_img.replace('jpg','png')\n",
        "        else:\n",
        "          source_path = PATH+\"/\"+dir+\"/\"+subdir+\"/\"+subset_img\n",
        "          destination_path = Subset_PATH+\"/\"+dir+\"/\"+subdir+\"/\"+subset_img\n",
        "        sh.copy(source_path, destination_path)\n",
        "  PATH = Subset_PATH"
      ],
      "metadata": {
        "id": "T2E84kyN-QqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining metrics"
      ],
      "metadata": {
        "id": "J876862WCVcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "\n",
        "def iou(y_true,y_pred):\n",
        "  y_pred = tf.argmax(y_pred,-1)\n",
        "  y_true = tf.argmax(y_true,-1)\n",
        "  tp = tf.cast(tf.reduce_sum(y_pred*y_true,axis=[1,2]), 'float32')\n",
        "  fp = tf.cast(tf.reduce_sum(y_pred*(1-y_true),axis=[1,2]), 'float32')\n",
        "  fn = tf.cast(tf.reduce_sum((1-y_pred)*y_true,axis=[1,2]), 'float32')\n",
        "  iou_value = (tp + 1e-14) / (tp + fp + fn + 1e-14)\n",
        "  return tf.reduce_mean(iou_value)\n",
        "\n",
        "def miou(y_true,y_pred):\n",
        "  y_pred = tf.argmax(y_pred,-1)\n",
        "  y_true = tf.argmax(y_true,-1)\n",
        "  tp = tf.cast(tf.reduce_sum(y_pred*y_true,axis=[1,2]), 'float32')\n",
        "  tn = tf.cast(tf.reduce_sum((1-y_pred)*(1-y_true),axis=[1,2]), 'float32')\n",
        "  fp = tf.cast(tf.reduce_sum(y_pred*(1-y_true),axis=[1,2]), 'float32')\n",
        "  fn = tf.cast(tf.reduce_sum((1-y_pred)*y_true,axis=[1,2]), 'float32')\n",
        "  iou1 = (tp + 1e-14) / (tp + fp + fn + 1e-14)\n",
        "  iou2 = (tn + 1e-14) / (tn + fp + fn + 1e-14)\n",
        "  iou_value = (iou1 + iou2) / 2\n",
        "  return tf.reduce_mean(iou_value)\n",
        "\n",
        "# Losses"
      ],
      "metadata": {
        "id": "JGxDUbzFp6IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from classification_models.tfkeras import Classifiers\n",
        "import pickle\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "O3mqAH-HyxjU",
        "outputId": "7cee647e-0de5-4f0c-b686-bc9d98682ed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "es_patience = 15\n",
        "rLR_patience = 5\n",
        "lr = 1e-2\n",
        "loss = 'categorical_crossentropy'\n",
        "BATCH_SIZE = 16\n",
        "metrics = [iou, 'accuracy']"
      ],
      "metadata": {
        "id": "3UI_uqydsmA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Augmentations"
      ],
      "metadata": {
        "id": "o8balmSsqI1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Canny"
      ],
      "metadata": {
        "id": "xcw3VO5RqLUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions for Model #1\n",
        "\n",
        "def canny_edge1(image):\n",
        "  l,u = 100, 200\n",
        "  edgesA = cv2.Canny(image[:,:,:3].numpy()[:,:,::-1], l, u)\n",
        "  edgesB = cv2.Canny(image[:,:,3:].numpy()[:,:,::-1], l, u)\n",
        "  edges = tf.stack([edgesA,edgesB],axis=-1)\n",
        "  edges = tf.cast(edges,tf.float32)\n",
        "  edges /= tf.reduce_max(edges)\n",
        "  return edges\n",
        "\n",
        "def canny_edge2(image):\n",
        "  l,u = 100, 200\n",
        "  eA = cv2.Canny(image[:,:,:3].numpy()[:,:,::-1], l, u) / 2.01\n",
        "  eB = cv2.Canny(image[:,:,3:].numpy()[:,:,::-1], l, u) / 2.01\n",
        "  e = eA-eB\n",
        "\n",
        "  edgesA = e>0\n",
        "  edgesB = e<0\n",
        "  edges = tf.stack([edgesA,edgesB],axis=-1)\n",
        "  edges = tf.cast(edges,tf.float32)\n",
        "  edges /= tf.reduce_max(edges)\n",
        "  return edges\n",
        "\n",
        "\n",
        "def load(imageA_file):\n",
        "  imageA = tf.io.read_file(imageA_file)\n",
        "  imageA = tf.image.decode_png(imageA)[:,:,:3]\n",
        "\n",
        "  imageB_file = tf.strings.regex_replace(imageA_file, f'/{pre_image}/', f'/{post_image}/')\n",
        "  imageB = tf.io.read_file(imageB_file)\n",
        "  imageB = tf.image.decode_jpeg(imageB)[:,:,:3]\n",
        "\n",
        "  label_file = tf.strings.regex_replace(imageA_file, f'/{pre_image}/', f'/{label_image}/')\n",
        "  label_file = tf.strings.regex_replace(label_file, '.jpg', '.png')\n",
        "  label = tf.io.read_file(label_file)\n",
        "  label = tf.image.decode_png(label)[:,:,0]\n",
        "\n",
        "  image = tf.concat([imageA,imageB],axis=-1)\n",
        "  edges = tf.py_function(canny_edge1, [image], tf.float32)\n",
        "  image = tf.cast(image,tf.float32)\n",
        "  label = tf.cast(label,tf.float32)\n",
        "  label = tf.stack([255-label,label],axis=-1)\n",
        "\n",
        "  return image, edges, label\n",
        "\n",
        "def normalize(image, label):\n",
        "  image = image / 255\n",
        "  label = label / 255\n",
        "  return image, label\n",
        "\n",
        "def load_image(image_file):\n",
        "  image, edges, label = load(image_file)\n",
        "  image, label = normalize(image, label)\n",
        "  image = tf.concat([image,edges],axis=-1)\n",
        "  return image, label\n",
        "\n",
        "train_dataset = tf.data.Dataset.list_files(f'{PATH}/train/{pre_image}/*.jpg')\n",
        "train_dataset = train_dataset.shuffle(1000)\n",
        "train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "train_dataset_length = len(train_dataset)\n",
        "# train_dataset_length = int(np.ceil(len(os.listdir(f'{PATH}/train/{pre_image}/'))/BATCH_SIZE))\n",
        "\n",
        "val_dataset = tf.data.Dataset.list_files(f'{PATH}/val/{pre_image}/*.jpg')\n",
        "val_dataset = val_dataset.map(load_image)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "val_dataset_length = len(val_dataset)\n",
        "# val_dataset_length = int(np.ceil(len(os.listdir(f'{PATH}/train/{pre_image}/'))/BATCH_SIZE))\n",
        "\n",
        "test_dataset = tf.data.Dataset.list_files(f'{PATH}/test/{pre_image}/*.jpg')\n",
        "test_dataset = test_dataset.map(load_image)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "test_dataset_length = len(test_dataset)\n",
        "# test_dataset_length = len(os.listdir(f'{PATH}/train/{pre_image}/'))\n",
        "\n",
        "print(f'train_dataset_length: {train_dataset_length}, val_dataset_length: {val_dataset_length}, test_dataset_length: {test_dataset_length}')"
      ],
      "metadata": {
        "id": "2amtol65p84Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dataset test\n",
        "# for x,y in train_dataset.take(1):\n",
        "#   pass\n",
        "# idx = 5\n",
        "# plt.figure(figsize=(20,4))\n",
        "# plt.subplot(151)\n",
        "# plt.imshow(x[idx,:,:,:3].numpy())\n",
        "# plt.subplot(152)\n",
        "# plt.imshow(x[idx,:,:,6].numpy(),cmap='gray')\n",
        "# plt.subplot(153)\n",
        "# plt.imshow(x[idx,:,:,3:6].numpy())\n",
        "# plt.subplot(154)\n",
        "# plt.imshow(x[idx,:,:,7].numpy(),cmap='gray')\n",
        "# plt.subplot(155)\n",
        "# plt.imshow(y[idx].numpy().argmax(-1),cmap='gray')"
      ],
      "metadata": {
        "id": "mQtAjIo3wObP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model #1\n",
        "def ef_model(input_shape = (256, 256, 8), backbone = 'resnet18'):\n",
        "\n",
        "  ResNet18, preprocess_input = Classifiers.get(backbone)\n",
        "  encoder = ResNet18(input_shape, weights=None,include_top=False)\n",
        "  encoder_output = encoder.output\n",
        "  skip_outputs = [encoder.get_layer(f).output for f in ['stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0']]\n",
        "\n",
        "  def conv_bn_relu(filters,x):\n",
        "    x = Conv2D(filters, 3, padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "  x = encoder_output\n",
        "  for i,skip_output in enumerate(skip_outputs):\n",
        "    x = UpSampling2D(name=f'up{i+1}')(x)\n",
        "    filters = skip_output.shape[-1]\n",
        "    x = conv_bn_relu(filters,x)\n",
        "    x = conv_bn_relu(filters,x)\n",
        "    x = concatenate([x, skip_output])\n",
        "  x = UpSampling2D(name='up5')(x)\n",
        "  for filters in [32,32,16,16]:\n",
        "    x = conv_bn_relu(filters,x)\n",
        "  x = Conv2D(2, 1, activation = 'softmax', padding='same', kernel_initializer='he_uniform', dtype = 'float32')(x)\n",
        "  model = Model(inputs = encoder.inputs, outputs = x)\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZPl8qiFC2Imh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model #1\n",
        "input_shape = (crop_size, crop_size, 8)\n",
        "backbone = 'resnet18'\n",
        "model = ef_model(input_shape, backbone)\n",
        "model_name = 'model_ef'\n",
        "model_path = f'saved/models/{dataset}_{backbone}_{model_name}_canny1.h5'\n",
        "history_path = f'saved/histories/{dataset}_{backbone}_{model_name}_canny1.pkl'\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_iou', mode='max', save_best_only=True, verbose = 1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_iou', mode = 'max', factor=1/np.sqrt(10), patience = rLR_patience, min_lr=1e-6, verbose = 1)\n",
        "earlystopper = EarlyStopping(monitor='val_iou', mode = 'max', patience = es_patience, verbose=1)\n",
        "callbacks = [checkpoint, reduce_lr, earlystopper]\n",
        "optimizer = Adam(learning_rate = lr)\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
        "hist = model.fit(train_dataset, epochs = epochs, validation_data = val_dataset, callbacks = callbacks)\n",
        "with open(history_path, 'wb') as file_pi:\n",
        "  pickle.dump(hist.history, file_pi)\n",
        "\n",
        "model.load_weights(model_path)\n",
        "_, test_iou, _ = model.evaluate(test_dataset)\n",
        "os.rename(model_path, model_path.replace('.h5',f'_{test_iou:.4f}.h5'))\n",
        "os.rename(history_path, history_path.replace('.pkl',f'_{test_iou:.4f}.pkl'))\n",
        "plt.plot(hist.history['iou'])\n",
        "plt.plot(hist.history['val_iou'])\n",
        "plt.title('model iou')\n",
        "plt.ylabel('iou')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yxktC7_R2Imi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MRA"
      ],
      "metadata": {
        "id": "jLEmfeqt7Ugh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions for Model #1\n",
        "\n",
        "def mra_haar(image):\n",
        "\n",
        "  decomp = pywt.wavedec2(cv2.cvtColor(image[:,:,:3].numpy(),cv2.COLOR_RGB2GRAY),wavelet='haar',mode='constant',level=1)\n",
        "  h_dA = decomp[1][0]\n",
        "  h_dA = (h_dA-h_dA.min())/(h_dA.max()-h_dA.min())\n",
        "  h_dA = cv2.resize(h_dA, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  v_dA = decomp[1][1]\n",
        "  v_dA = (v_dA-v_dA.min())/(v_dA.max()-v_dA.min())\n",
        "  v_dA = cv2.resize(v_dA, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  d_dA = decomp[1][2]\n",
        "  d_dA = (d_dA-d_dA.min())/(d_dA.max()-d_dA.min())\n",
        "  d_dA = cv2.resize(d_dA, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "\n",
        "  decomp = pywt.wavedec2(cv2.cvtColor(image[:,:,3:].numpy(),cv2.COLOR_RGB2GRAY),wavelet='haar',mode='constant',level=1)\n",
        "  h_dB = decomp[1][0]\n",
        "  h_dB = (h_dB-h_dB.min())/(h_dB.max()-h_dB.min())\n",
        "  h_dB = cv2.resize(h_dB, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  v_dB = decomp[1][1]\n",
        "  v_dB = (v_dB-v_dB.min())/(v_dB.max()-v_dB.min())\n",
        "  v_dB = cv2.resize(v_dB, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  d_dB = decomp[1][2]\n",
        "  d_dB = (d_dB-d_dB.min())/(d_dB.max()-d_dB.min())\n",
        "  d_dB = cv2.resize(d_dB, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  edges = tf.stack([h_dA, v_dA, d_dA, h_dB, v_dB, d_dB],axis=-1)\n",
        "  edges = tf.cast(edges,tf.float32)\n",
        "\n",
        "  return edges\n",
        "\n",
        "def mra_db7(image):\n",
        "\n",
        "  decomp = pywt.wavedec2(cv2.cvtColor(image[:,:,:3].numpy(),cv2.COLOR_RGB2GRAY),wavelet='db7',mode='constant',level=1)\n",
        "  h_dA = decomp[1][0]\n",
        "  h_dA = (h_dA-h_dA.min())/(h_dA.max()-h_dA.min())\n",
        "  h_dA = cv2.resize(h_dA, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  v_dA = decomp[1][1]\n",
        "  v_dA = (v_dA-v_dA.min())/(v_dA.max()-v_dA.min())\n",
        "  v_dA = cv2.resize(v_dA, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  d_dA = decomp[1][2]\n",
        "  d_dA = (d_dA-d_dA.min())/(d_dA.max()-d_dA.min())\n",
        "  d_dA = cv2.resize(d_dA, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "\n",
        "  decomp = pywt.wavedec2(cv2.cvtColor(image[:,:,3:].numpy(),cv2.COLOR_RGB2GRAY),wavelet='db7',mode='constant',level=1)\n",
        "  h_dB = decomp[1][0]\n",
        "  h_dB = (h_dB-h_dB.min())/(h_dB.max()-h_dB.min())\n",
        "  h_dB = cv2.resize(h_dB, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  v_dB = decomp[1][1]\n",
        "  v_dB = (v_dB-v_dB.min())/(v_dB.max()-v_dB.min())\n",
        "  v_dB = cv2.resize(v_dB, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  d_dB = decomp[1][2]\n",
        "  d_dB = (d_dB-d_dB.min())/(d_dB.max()-d_dB.min())\n",
        "  d_dB = cv2.resize(d_dB, (crop_size,crop_size), cv2.INTER_LINEAR)\n",
        "  edges = tf.stack([h_dA, v_dA, d_dA, h_dB, v_dB, d_dB],axis=-1)\n",
        "  edges = tf.cast(edges,tf.float32)\n",
        "\n",
        "  return edges\n",
        "\n",
        "\n",
        "def load(imageA_file):\n",
        "  imageA = tf.io.read_file(imageA_file)\n",
        "  imageA = tf.image.decode_png(imageA)[:,:,:3]\n",
        "\n",
        "  imageB_file = tf.strings.regex_replace(imageA_file, f'/{pre_image}/', f'/{post_image}/')\n",
        "  imageB = tf.io.read_file(imageB_file)\n",
        "  imageB = tf.image.decode_jpeg(imageB)[:,:,:3]\n",
        "\n",
        "  label_file = tf.strings.regex_replace(imageA_file, f'/{pre_image}/', f'/{label_image}/')\n",
        "  label_file = tf.strings.regex_replace(label_file, '.jpg', '.png')\n",
        "  label = tf.io.read_file(label_file)\n",
        "  label = tf.image.decode_png(label)[:,:,0]\n",
        "\n",
        "  image = tf.concat([imageA,imageB],axis=-1)\n",
        "  edges = tf.py_function(mra_haar, [image], tf.float32)\n",
        "  image = tf.cast(image,tf.float32)\n",
        "  label = tf.cast(label,tf.float32)\n",
        "  label = tf.stack([255-label,label],axis=-1)\n",
        "\n",
        "  return image, edges, label\n",
        "\n",
        "def normalize(image, label):\n",
        "  image = image / 255\n",
        "  label = label / 255\n",
        "  return image, label\n",
        "\n",
        "def load_image(image_file):\n",
        "  image, edges, label = load(image_file)\n",
        "  image, label = normalize(image, label)\n",
        "  image = tf.concat([image,edges],axis=-1)\n",
        "  return image, label\n",
        "\n",
        "train_dataset = tf.data.Dataset.list_files(f'{PATH}/train/{pre_image}/*.jpg')\n",
        "train_dataset = train_dataset.shuffle(1000)\n",
        "train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "train_dataset_length = len(train_dataset)\n",
        "# train_dataset_length = int(np.ceil(len(os.listdir(f'{PATH}/train/{pre_image}/'))/BATCH_SIZE))\n",
        "\n",
        "val_dataset = tf.data.Dataset.list_files(f'{PATH}/val/{pre_image}/*.jpg')\n",
        "val_dataset = val_dataset.map(load_image)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "val_dataset_length = len(val_dataset)\n",
        "# val_dataset_length = int(np.ceil(len(os.listdir(f'{PATH}/train/{pre_image}/'))/BATCH_SIZE))\n",
        "\n",
        "test_dataset = tf.data.Dataset.list_files(f'{PATH}/test/{pre_image}/*.jpg')\n",
        "test_dataset = test_dataset.map(load_image)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "test_dataset_length = len(test_dataset)\n",
        "# test_dataset_length = len(os.listdir(f'{PATH}/train/{pre_image}/'))\n",
        "\n",
        "print(f'train_dataset_length: {train_dataset_length}, val_dataset_length: {val_dataset_length}, test_dataset_length: {test_dataset_length}')"
      ],
      "metadata": {
        "id": "BQE6Z5EkKoVl",
        "outputId": "c9c88c53-5180-4317-8007-57a38b3ae8fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_length: 128, val_dataset_length: 32, test_dataset_length: 87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dataset test\n",
        "# for x,y in train_dataset.take(1):\n",
        "#   pass\n",
        "# idx = 5\n",
        "# plt.figure(figsize=(40,16))\n",
        "# plt.subplot(251)\n",
        "# plt.imshow(x[idx,:,:,:3].numpy())\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(252)\n",
        "# plt.imshow(x[idx,:,:,6].numpy(),cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(256)\n",
        "# plt.imshow(x[idx,:,:,7].numpy(),cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(257)\n",
        "# plt.imshow(x[idx,:,:,8].numpy(),cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(253)\n",
        "# plt.imshow(x[idx,:,:,3:6].numpy())\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(254)\n",
        "# plt.imshow(x[idx,:,:,9].numpy(),cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(258)\n",
        "# plt.imshow(x[idx,:,:,10].numpy(),cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(259)\n",
        "# plt.imshow(x[idx,:,:,11].numpy(),cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(255)\n",
        "# plt.imshow(y[idx].numpy().argmax(-1),cmap='gray')\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout()"
      ],
      "metadata": {
        "id": "ieFgpbrtTE6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model #1\n",
        "def ef_model(input_shape = (256, 256, 8), backbone = 'resnet18'):\n",
        "\n",
        "  ResNet18, preprocess_input = Classifiers.get(backbone)\n",
        "  encoder = ResNet18(input_shape, weights=None,include_top=False)\n",
        "  encoder_output = encoder.output\n",
        "  skip_outputs = [encoder.get_layer(f).output for f in ['stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0']]\n",
        "\n",
        "  def conv_bn_relu(filters,x):\n",
        "    x = Conv2D(filters, 3, padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "  x = encoder_output\n",
        "  for i,skip_output in enumerate(skip_outputs):\n",
        "    x = UpSampling2D(name=f'up{i+1}')(x)\n",
        "    filters = skip_output.shape[-1]\n",
        "    x = conv_bn_relu(filters,x)\n",
        "    x = conv_bn_relu(filters,x)\n",
        "    x = concatenate([x, skip_output])\n",
        "  x = UpSampling2D(name='up5')(x)\n",
        "  for filters in [32,32,16,16]:\n",
        "    x = conv_bn_relu(filters,x)\n",
        "  x = Conv2D(2, 1, activation = 'softmax', padding='same', kernel_initializer='he_uniform', dtype = 'float32')(x)\n",
        "  model = Model(inputs = encoder.inputs, outputs = x)\n",
        "  return model"
      ],
      "metadata": {
        "id": "OktzedzNZ2nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model #1\n",
        "input_shape = (crop_size, crop_size, 12)\n",
        "backbone = 'resnet18'\n",
        "model = ef_model(input_shape, backbone)\n",
        "model_name = 'model_ef'\n",
        "model_path = f'saved/models/{dataset}_{backbone}_{model_name}_mra_haar.h5'\n",
        "history_path = f'saved/histories/{dataset}_{backbone}_{model_name}_mra_haar.pkl'\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_iou', mode='max', save_best_only=True, verbose = 1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_iou', mode = 'max', factor=1/np.sqrt(10), patience = rLR_patience, min_lr=1e-6, verbose = 1)\n",
        "earlystopper = EarlyStopping(monitor='val_iou', mode = 'max', patience = es_patience, verbose=1)\n",
        "callbacks = [checkpoint, reduce_lr, earlystopper]\n",
        "optimizer = Adam(learning_rate = lr)\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
        "hist = model.fit(train_dataset, epochs = epochs, validation_data = val_dataset, callbacks = callbacks)\n",
        "with open(history_path, 'wb') as file_pi:\n",
        "  pickle.dump(hist.history, file_pi)\n",
        "\n",
        "model.load_weights(model_path)\n",
        "_, test_iou, _ = model.evaluate(test_dataset)\n",
        "os.rename(model_path, model_path.replace('.h5',f'_{test_iou:.4f}.h5'))\n",
        "os.rename(history_path, history_path.replace('.pkl',f'_{test_iou:.4f}.pkl'))\n",
        "plt.plot(hist.history['iou'])\n",
        "plt.plot(hist.history['val_iou'])\n",
        "plt.title('model iou')\n",
        "plt.ylabel('iou')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hml5MxWiZ2nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Move saved files to drive"
      ],
      "metadata": {
        "id": "U-m9yIClfc0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r saved.zip saved/\n",
        "!cp saved.zip /content/drive/MyDrive/saved.zip"
      ],
      "metadata": {
        "id": "uzAE3zMJbMtN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}